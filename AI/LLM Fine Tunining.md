## LLM Fine-Tuning Tutorial (Step-by-Step)

### What is LLM Fine-Tuning?

Fine-tuning = training a pre-trained Large Language Model (LLM) on your own data so it:
- Speaks in your domain
- Follows your style
- Performs a specific task better

Example

| Task	| Without Fine-Tuning	| With Fine-Tuning | 
| ----- | ------------ | ---------- | 
| Medical Q&A | Generic answers	| Doctor-like responses | 
| Chatbot | Casual | Company tone | 
| Code generation | General	| Your codebase style | 

### When Should You Fine-Tune?

‚úÖ Fine-tune if:
- You need domain-specific language
- You want consistent structured output
- Prompt engineering alone is not enough

‚ùå Don‚Äôt fine-tune if:
- You only need factual answers ‚Üí use RAG
- Data is small (<100 examples) ‚Üí use prompts

### Types of Fine-Tuning
1. Full Fine-Tuning ‚ùå (Expensive)
- Updates all model weights
- Needs huge GPU (A100)

2. Parameter-Efficient Fine-Tuning (PEFT) ‚úÖ (Recommended)
- Updates only a small % of weights
- Much cheaper & faster

Popular PEFT methods
- LoRA ‚≠ê
- QLoRA ‚≠ê‚≠ê
- Adapters

üëâ We‚Äôll use LoRA.

### Tech Stack (What We‚Äôll Use)

- Python 3.10+
- PyTorch
- Hugging Face Transformers
- Datasets
- PEFT (LoRA)


## LLM Fine-Tuning Tutorial (Google Colab Edition)

**Goal:** Fine-tune a small LLM using LoRA on Google Colab
**Model:** TinyLlama/TinyLlama-1.1B-Chat-v1.0
**Use case:** Instruction-based chatbot

### 0Ô∏è‚É£ Colab Setup (IMPORTANT)

Step 0.1 ‚Äì Enable GPU
- Colab ‚Üí Runtime
- Change runtime type
- Hardware accelerator ‚Üí GPU (T4)

### 1Ô∏è‚É£ Install Dependencies (Colab-Safe)

```bash
pip install -q torch transformers datasets peft accelerate bitsandbytes
```

Restart runtime if prompted.

### 2Ô∏è‚É£ Why TinyLlama?
| Feature | 	Reason | 
| ------ |  ------- | 
| Size	| 1.1B (fits free GPU) | 
| License | Open (no token) | 
| Architecture | LLaMA-like | 
| Training speed	| Fast | 

### 3Ô∏è‚É£ Prepare Training Dataset

Instruction-Tuning Format

Create a dataset where the model learns to:

Follow instructions and generate responses

Example Dataset

```python
import json

data = [
    {
        "instruction": "What is Python?",
        "output": "Python is a high-level, interpreted programming language."
    },
    {
        "instruction": "Explain list comprehension in Python.",
        "output": "List comprehension provides a concise way to create lists using expressions."
    },
    {
        "instruction": "What is a decorator in Python?",
        "output": "A decorator modifies the behavior of a function using the @ syntax."
    }
]

with open("train.json", "w") as f:
    json.dump(data, f, indent=2)

```

### 4Ô∏è‚É£ Load Model & Tokenizer
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,
    device_map="auto"
)

```

‚úÖ Works on free Colab
‚úÖ No authentication needed

### 5Ô∏è‚É£ Apply LoRA (Core Fine-Tuning Step)

Why LoRA?

- Train <1% parameters
- Faster
- Memory efficient

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
```

Expected output:

```text
trainable params: ~4M
total params: ~1.1B
```

### 6Ô∏è‚É£ Load & Tokenize Dataset

```python
from datasets import load_dataset

dataset = load_dataset("json", data_files="train.json")

def format_prompt(example):
    return f"""### Instruction:
{example['instruction']}

### Response:
{example['output']}"""

def tokenize(example):
    prompt = format_prompt(example)
    return tokenizer(
        prompt,
        truncation=True,
        padding="max_length",
        max_length=512
    )

tokenized_dataset = dataset.map(tokenize)

7Ô∏è‚É£ Training Configuration (Colab-Optimized)
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./llm-finetune",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=3,
    fp16=True,
    logging_steps=5,
    save_strategy="epoch",
    report_to="none"
)

8Ô∏è‚É£ Train the Model üöÄ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"]
)

trainer.train()


‚è±Ô∏è Training time: 5‚Äì15 minutes

9Ô∏è‚É£ Test the Fine-Tuned Model
def generate(text):
    prompt = f"""### Instruction:
{text}

### Response:
"""
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=100
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(generate("Explain generators in Python"))


üéâ You just fine-tuned an LLM on Colab!

üîç What Actually Happened?
Step	Meaning
Pretrained model	General knowledge
LoRA	Injected task-specific behavior
Dataset	Taught instruction following
Trainer	Updated LoRA weights only
üß† Common Colab Mistakes (Avoid These)

‚ùå Using LLaMA-2 without token
‚ùå Batch size too big
‚ùå Full fine-tuning
‚ùå Too little data (<50 samples)

üîÅ Fine-Tuning vs RAG (Quick Interview Table)
Feature	Fine-Tuning	RAG
Style learning	‚úÖ	‚ùå
Knowledge update	‚ùå	‚úÖ
Cost	High	Low
Hallucination control	Medium	High
